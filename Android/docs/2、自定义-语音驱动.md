## 流程说明

Avatar 有一个属性是 `BlendShape`，它是一个长度 67 的、取值范围 0 到 1 的 Float 数组。由它来控制 Avatar 的表情。

序号0-45代表基表情bs，46-56代表口腔bs，57-66代表舌头bs。

语音驱动的流程是：

首先通过 ASR 服务得到要驱动的音频文件和其音素时间戳；

然后将音素时间戳调用 SDK 的 sta 相关接口，得到对应的每个时刻的 BS 嘴型，并将结果缓存；

然后播放音频文件，在渲染画面时获取当前播放进度，拿到此刻应该显示的 BS 数据，将其设置给 Avatar。

## 渲染接口

这部分控制的是具体的嘴型如何驱动。

```kotlin
//设置是否开启 BS 控制。true 则可以通过设置 BS 的方式驱动脸型。
avatar.blendShape.setEnableExpressionBlend(enable: Boolean)
//设置 BS 数据。即希望此刻显示的嘴型
avatar.blendShape.setInputBlendShape(expression: FloatArray)
//设置表情系数（BS）的权重。取值 0 到 1，对指定区域设为 0 即关闭对应表情。
avatar.blendShape.setSystemBlendShapeWeight(widgetArray: FloatArray)
```

对于 Avatar 来说一次语音驱动的流程就是：

1、播放开始时，设置开启 BS 控制和相关权重。

```kotlin
val mouthAnimationDisableWidget: FloatArray by lazy {//关闭
    val array = FloatArray(mPhonemeCoefficientLength) { 1f }
    array.forEachIndexed { index, _ ->
        if (index > 20 && index != 42) {
            array[index] = 0f
        }
    }
    array
}

avatar.blendShape.setEnableExpressionBlend(true)
avatar.blendShape.setSystemBlendShapeWeight(mouthAnimationDisableWidget)
```

2、播放中，获取到此刻对应的 BS 数据，设置。

```kotlin
avatar.blendShape.setInputBlendShape(expression)
```

3、结束播放，关闭 BS 控制和重设相关权重。

```kotlin
val mouthAnimationWidget by lazy { FloatArray(mPhonemeCoefficientLength) { 1f } } //开启

avatar.blendShape.setEnableExpressionBlend(false)
avatar.blendShape.setSystemBlendShapeWeight(mouthAnimationWidget)
```



需要注意的是，这些方法默认是由内部线程调度的，因此为了确保效果能精准地对上这一帧，这些方法最好在 GL 线程中调用（如相芯提供的 Renderer 的 onRenderBefore 方法），并将上述方法的 `needBackgroundThread` 参数传入 false。

## STA 接口

这部分是控制如何将音素时间戳转换为对应的 BS 数据。

需要引入仓库：`com.faceunity.gpb:sta`

具体的能力通过 `FUStaKit` 调用，相关接口说明参考接口文档。通常直接使用它的封装 `StaProcessingModuleSync` 即可。

```kotlin
//StaProcessingModuleSync 主要的几个方法

//初始化
fun initStaEngine(staPack: ByteArray, langType: FULangTypeEnum, configData: ByteArray, decoder: ByteArray)
//流式地将音素时间戳传入，并将结果缓存。
fun appendPhonemesFromAudio(stream: ByteArray, streamType: FUStreamTypeEnum, langType: FULangTypeEnum, streamState: FUAudioProgressTypeEnum)
//非流式地将音素时间戳传入，并将结果缓存。
fun resetPhonemesFromAudio(stream: ByteArray, streamType: FUStreamTypeEnum, langType: FULangTypeEnum, streamState: FUAudioProgressTypeEnum)
//根据进度获取缓存的 BS 数据。
fun getExpressionByAudioPosition(audioPosition: Long): FloatArray?
```

## VTA 接口

这部分是控制如何将音频文件解析成音素时间戳。可选集成。

需要引入仓库：`com.faceunity.gpb:vta`

由于在 Demo 中，对应的音频数据和音素时间戳都由云端提供，不需要这部分内容，故暂不说明。查看 `FUVtaKit` 的接口文档即可。

## 封装接口

在 Factory 和 Demo 中，渲染和驱动由客户端完成，而 ASR、NLP、TTS 等服务由云端实现。故 Factory SDK 将其封装为了 `ISTARenderControl`、`ISTAServiceControl` 两套接口，并有其对应实现。

### STA 渲染接口 ISTARenderControl

它封装了上述渲染接口、STA 接口，使调用者可直接传入音素时间戳和音频数据进行驱动。

```kotlin
//ISTARenderControl 主要的几个方法

fun initRender(context: Context, initSpeechConfig: InitSpeechConfig)

fun setControlAvatar(avatar: Avatar)
/**
 * 将传入的音素时间戳（[phonemeTimestamp]）与音频文件（[audioData]）进行播报。
 */
fun playSpeech(id: String, phonemeTimestamp: String, audioData: ByteArray, timestampType: String = "phoneme", speechCallback: SpeechCallback? = null)

/**
 * 将传入的流式音素时间戳（[phonemeTimestamp]）与音频文件（[audioData]）进行播报。
 * @param streamStatus 当前流的状态，开始、中间或结束。
 */
fun playSpeechStream(id: String, phonemeTimestamp: String, audioData: ByteArray, timestampType: String = "phoneme", streamStatus: StreamStatus, speechCallback: SpeechCallback? = null)
/**
 * 将传入的流式音素时间戳（[phonemeTimestamp]）与音频文件（[audioData]）以叠加的方式进行播报。
 * 相对 [playSpeechStream] 的区别在于，传入的音素时间戳（[phonemeTimestamp]）不是直接设置给底层接口，而是进行一个累加，再通过非流式的方式调用。
 * @param streamStatus 当前流的状态，开始、中间或结束。
 */
fun playSpeechStreamOverlay(id: String, phonemeTimestamp: String, audioData: ByteArray, timestampType: String = "phoneme", streamStatus: StreamStatus, speechCallback: SpeechCallback? = null)
/**
 * 同步音频的播报状态，需要在 Renderer 的 onRenderAfter 方法中调用。
 */
fun syncSpeechStatus()
/**
 * 取消指定 [id] 的播报，如果传 null 则取消当前正在播报的 id。
 */
fun stopSpeech(id: String? = null)
/**
 * 取消所有播报，会调用对应的 [SpeechCallback.onCancel]。
 */
fun cancelAllSpeech()
/**
 * 将在 [setControlAvatar] 设置的 Avatar 置空。
 */
fun releaseControlAvatar()
/**
 * 释放播放器、STA 等的缓存。
 */
fun releaseRender()
```

大致的流程是：

准备阶段：调用 initRender 初始化、调用 setControlAvatar 指定要驱动的 Avatar、在 Renderer 的 onRenderBefore 中调用 syncSpeechStatus 方法。

驱动阶段：根据数据源类型，分为调用非流式（playSpeech）、流式（playSpeechStream）、流式叠加式（playSpeechStreamOverlay）三种，选择一种传入数据。

结束阶段：调用 releaseRender 释放相关资源。



其中，id 是调用者自定义的，用于回调区分结果和取消指定播报。

三种传入方式的区别是：`playSpeech` 是服务端返回非流式数据，调用 SDK 的非流式接口得到结果；`playSpeechStream` 是服务端返回流式数据，调用 SDK 的流式接口（appendPhonemesFromAudio）得到结果；`playSpeechStreamOverlay` 是服务端返回流式数据，调用 SDK 的非流式接口（resetPhonemesFromAudio）得到结果。

`playSpeechStreamOverlay`目前得到的结果最准确，但更耗性能。



该接口的默认实现为 `STARenderControlImpl`。在 Demo 中通过 **FuDependencyInjection.staModel** 指定，接入者可参考其实现，定制符合自己需求的实现方式。（比如指定播放器等）



### STA 服务接口 ISTAServiceControl

它封装了具体的业务场景，使调用者可直接实现对应效果。

```kotlin
// ISTAServiceControl 的几个主要方法

fun initService()
fun setSTARenderControl(starRenderControl: ISTARenderControl)
/**
 * 将传入的文本（[speechContent]）和指定音色（[voice]），经过 TTS 服务，转为 Avatar 的语音播报。
 */
fun startSpeech(speechContent: String, voice: String, speechCallback: SpeechCallback? = null, ttsCallback: TtsCallback? = null)
/**
 * 将传入的对话（[inputContent]）经过 NLP 服务，得到对应回答，再通过指定音色（[voice]）进行 Avatar 的语音播报。
 */
fun startChat(inputContent: String, voice: String, speechCallback: SpeechCallback? = null, nlpTtsCallback: NlpTtsCallback? = null)
/**
 * 将传入的音频（[inputPcmData]）经过 ASR 服务解析对应对话，再经过 NLP 服务，得到对应回答，再通过指定音色（[voice]）进行 Avatar 的语音播报。
 */
fun startChat(inputPcmData: ByteArray, voice: String, speechCallback: SpeechCallback? = null, asrNlpTtsCallback: AsrNlpTtsCallback? = null)
/**
 * 取消掉对后续结果的响应
 */
fun cancelAccept()
fun releaseService()
```

因为互动模块的云服务暂不对外，故对应具体实现暂不赘述，接入者可根据对应需求实现对应接口。
